{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Test Set \n",
    "\n",
    "This script assumes that the test dataset filename is `ner_test.txt`, that it is in a folder called Datasets (as this is the folder in which the training data was placed), and that the script is in the parent folder. These parameters can easily be modified should these assumptions not hold true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_classifier = open('ne_clf_svc.pickle', 'rb')\n",
    "ne_clf = pickle.load(saved_classifier) \n",
    "saved_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(filepath):\n",
    "    \n",
    "    df = pd.read_csv(filepath, sep=' ', header=None)\n",
    "    df.columns = ['token', 'pos_tag', 'chunk_tag', 'ne_tag']\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    y = df['ne_tag']\n",
    "    X = []\n",
    "\n",
    "    def features(token, index, pos_tag, ne_tag):\n",
    "        first_letter = token[index][0]\n",
    "        features = {'token': token[index],\n",
    "                    'pos': pos_tag[index],\n",
    "                    'prev_token': '' if index == 0 else token[index-1],\n",
    "                    'prev_pos': '' if index == 0 else pos_tag[index-1],\n",
    "                    'prev_ne': '' if index == 0 else ne_tag[index-1],\n",
    "                    'next_token': '' if index == len(token)-1 else token[index+1],\n",
    "                    'next_pos': '' if index == len(token)-1 else pos_tag[index+1],\n",
    "                    'prev_prev_token': '' if index == 0 or index == 1 else token[index-2],\n",
    "                    'prev_prev_pos': '' if index == 0 or index == 1 else pos_tag[index-2],\n",
    "                    'next_next_token': '' if index == len(token)-1 or index == len(token)-2 else token[index+2],\n",
    "                    'next_next_pos': '' if index == len(token)-1 or index == len(token)-2 else pos_tag[index+2],\n",
    "                    'is_capitalized': first_letter.upper() in string.ascii_uppercase and first_letter.upper() == first_letter,\n",
    "                    'is_numeric': token[index].isdigit(),                \n",
    "                    'is_all_caps': token[index].upper() == token[index],\n",
    "                    'caps_inside': token[index][1:].lower() != token[index][1:]\n",
    "                    }\n",
    "        return features\n",
    "\n",
    "    for index in range(len(df.token)):\n",
    "        X.append(features(df.token, index, df.pos_tag, df.ne_tag))\n",
    "    \n",
    "    predicted = ne_clf.predict(X)  \n",
    "    print(metrics.classification_report(y, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Datasets/ner_test.txt'\n",
    "predict_on_test_set(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_paragraphs = ['She vowed to get the deal signed off in Brussels and put it to a vote of MPs.',\n",
    "'It follows a string of ministerial resignations and talk of a no-confidence vote from Tory MPs.',\n",
    "'Brexit Secretary Dominic Raab and Work and Pensions Secretary Esther McVey both quit earlier in protest at the withdrawal agreement, along with two junior ministers.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_texts(texts):\n",
    "    \n",
    "    texts = ' '.join(texts) \n",
    "    texts = texts.replace('\\n',' ') \n",
    "    tokens = word_tokenize(texts) \n",
    "    token_pos_tag = pos_tag(tokens) \n",
    "    ne_tag_hist = []\n",
    "    \n",
    "    def pos(token_and_tag): \n",
    "        return [t[1] for t in token_and_tag]\n",
    "    \n",
    "    def features(token, index, pos_tag, ne_tag_hist):\n",
    "        first_letter = token[index][0]\n",
    "        features = {'token': token[index],\n",
    "                    'pos': pos_tag[index],\n",
    "                    'prev_token': '' if index == 0 else token[index-1],\n",
    "                    'prev_pos': '' if index == 0 else pos_tag[index-1],\n",
    "                    'prev_ne': '' if index == 0 else ne_tag_hist[index-1],\n",
    "                    'next_token': '' if index == len(token)-1 else token[index+1],\n",
    "                    'next_pos': '' if index == len(token)-1 else pos_tag[index+1],\n",
    "                    'prev_prev_token': '' if index == 0 or index == 1 else token[index-2],\n",
    "                    'prev_prev_pos': '' if index == 0 or index == 1 else pos_tag[index-2],\n",
    "                    'next_next_token': '' if index == len(token)-1 or index == len(token)-2 else token[index+2],\n",
    "                    'next_next_pos': '' if index == len(token)-1 or index == len(token)-2 else pos_tag[index+2],\n",
    "                    'is_capitalized': first_letter.upper() in string.ascii_uppercase and first_letter.upper() == first_letter,\n",
    "                    'is_numeric': token[index].isdigit(),                \n",
    "                    'is_all_caps': token[index].upper() == token[index],\n",
    "                    'caps_inside': token[index][1:].lower() != token[index][1:]\n",
    "                    }\n",
    "        return features\n",
    "    \n",
    "    for index in range(len(tokens)):\n",
    "        X = features(tokens, index, pos(token_pos_tag), ne_tag_hist)\n",
    "        predicted = ne_clf.predict(X)\n",
    "        ne_tag_hist.append(predicted[0]) \n",
    "\n",
    "    for token, predicted_tag in zip(tokens, ne_tag_hist):\n",
    "        print('%s => %s' % (token, predicted_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_on_texts(news_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
