{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.25)\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Import and Examine Dataset\n",
    "\n",
    "There are 211,727 data points and no missing values. The classes are very imbalanced.\n",
    "- NN is the most frequent POS tag (30,147 occurrences). SYM is the least frequent (six occurrences). \n",
    "- I-NP is the most frequent chunk tag (63,307 occurrences). B-UCP and I-PRT are the least frequent (two occurrences each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunking_data = 'Datasets/chunking_dataset.txt'\n",
    "df = pd.read_csv(chunking_data, sep=' ', header=None)\n",
    "df.columns = ['token', 'pos_tag', 'chunk_tag']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pos_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.title('POS Tag Frequency')\n",
    "sns.countplot(data=df, x='pos_tag');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.chunk_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.title('Chunk Tag Frequency')\n",
    "sns.countplot(data=df, x='chunk_tag');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine tokens labelled as each of the chunk tags to understand their characteristics. This is important for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.chunk_tag=='B-CONJP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Engineering and Prepare Training and Validation Sets\n",
    "\n",
    "A baseline model would only have tokens and POS tags as input features, and would select the chunk tag most frequently associated with each POS tag (tokens are needed to predict the POS tags).\n",
    "\n",
    "The more sophisticated approach adopted here considers the *context* of each token: the words preceding and following it, as well as their POS tags. This makes linguistic sense - articles such as 'the' and 'a' come before nouns and form chunks with them; 'to' often comes before verbs. Thus, context also allows for distinctions to be drawn between [words that can be verbs and nouns](https://www.enchantedlearning.com/wordlist/nounandverb.shtml).\n",
    "\n",
    "Different combinations of features were tested and validated with the Decision Tree classifier, as it is arguably the most transparent. To save time, the classifier was tested on only 10,000 tokens. Apart from tokens and POS tags, stemming (a non-lexicographical representation of the root form of a word), capitalisation, and tokens up to two indices before or after were considered, along with their POS tags. The previous chunk tag was also added.\n",
    " \n",
    "Here are the mean F1 scores:\n",
    "- token, POS: 0.7892409455950302\n",
    "- token, stem, POS: 0.7894436549142528\n",
    "- token, POS, previous token/POS: 0.9072529562030741\n",
    "- token, POS, previous token/POS, next token/POS: 0.9170758996482626\n",
    "- token, POS, previous token/POS, next token/POS, previous previous token/POS: 0.9162323510943451\n",
    "- token, POS, previous token/POS, next token/POS, previous previous token/POS, next next token/POS: 0.9113964680597613\n",
    "- POS, previous POS, next POS, previous chunk: 0.9255171443010305\n",
    "- token, POS, previous token/POS, next token/POS, previous chunk, is_capitalized: 0.9363727091377456\n",
    "- <b>*token, POS, previous token/POS, next token/POS, previous chunk: 0.938063043809734*</b>\n",
    "\n",
    "The best combination is bolded and italicised, and was used in the classifier models. Stemming, capitalisation, and a two-token context window do not make a difference. Stemming and capitalisation overlap too much with the POS tag (they would be far more useful in a model that does not include the POS tag). Tokens two words before or after are probably not helpful due to the training sample size of 10,000 tokens (5% of the full training data), as there aren't enough instances of particular long chunk patterns in this small sample.\n",
    "\n",
    "A 90:10 ratio was used to create test and validation datasets from the training dataset, to reflect the proportions used in 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "def features(token, index, pos_tag, chunk_tag):\n",
    "#    first_letter = token[index][0]\n",
    "    features = {'token': token[index],\n",
    "                'pos': pos_tag[index],\n",
    "                'prev_token': '' if index == 0 else token[index-1],\n",
    "                'prev_pos': '' if index == 0 else pos_tag[index-1],\n",
    "                'prev_chunk': '' if index == 0 else chunk_tag[index-1],\n",
    "                'next_token': '' if index == len(df.token)-1 else token[index+1],\n",
    "                'next_pos': '' if index == len(df.token)-1 else pos_tag[index+1]\n",
    "#                'prev_prev_token': '' if index == 0 or index == 1 else token[index-2],\n",
    "#                'prev_prev_pos': '' if index == 0 or index == 1 else pos_tag[index-2],\n",
    "#                'is_capitalized': first_letter.upper() in string.ascii_uppercase and first_letter.upper() == first_letter,\n",
    "#                'next_next_token': '' if index == len(df.token)-1 or index == len(df.token)-2 else token[index+2],\n",
    "#                'next_next_pos': '' if index == len(df.token)-1 or index == len(df.token)-2 else pos_tag[index+2]\n",
    "                }\n",
    "    return features\n",
    "    \n",
    "X = []\n",
    "\n",
    "for index in range(len(df.token)):\n",
    "    X.append(features(df.token, index, df.pos_tag, df.chunk_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5] # Confirm that the features are complete and accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['chunk_tag']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"Size of training set (POS tags):\", len(X_train)) \n",
    "print(\"Size of test set (POS tags):\", len(X_test)) \n",
    "print(\"Size of training set (chunk tags):\", len(y_train)) \n",
    "print(\"Size of test set (chunk tags):\", len(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Classifier Training and Hyperparameter Tuning\n",
    "\n",
    "Five classifiers were selected due to their popular use in supervised classification tasks for multiclass data.\n",
    "\n",
    "**Decision Tree**: this is the most easily interpretable model, given that it can be represented as a series of logical rules (if-then statements). \n",
    "- Entropy was chosen over the default Gini impurity for the criterion argument (which measures the quality of a split), because it is suited for nominative as opposed to continuous attributes.\n",
    "\n",
    "**Multinomial Na√Øve Bayes**: another simple classifier for discrete data that is computationally very lightweight and biased toward the modal category. This variant is suited for occurrence counts (as opposed to Boolean features).\n",
    "\n",
    "**Logistic Regression (Maximum Entropy)**: with its default settings, this is also a computationally lightweight algorithm. It considers probability distributions that are 'empirically consistent' with the training data (estimated frequencies of class and feature vector co-occurrences are close to actual frequencies). The distribution with the highest entropy (i.e., the most uniform) is selected. \n",
    "- As classes are highly imbalanced in this dataset (as demonstrated by the exploratory analysis above), the 'balanced' option of the `class_weight` argument is selected to automatically adjust weights inversely proportional to class frequencies in the input data.\n",
    "- I experimented with the SAGA solver, a variant of Stochastic Average Gradient descent that supports L1 Regularization. It is therefore the solver of choice for sparse multinomial logistic regression, and is suitable for large datasets. However, this version was far too computationally intense, so I reverted back to liblinear (the default - not as ideal for multiclass, but still achieves quite high accuracies).\n",
    "\n",
    "**Support Vector Machine (LinearSVC)**: in my personal experience with text classification this is the best-performing model, so I had to test it! LinearSVC is an effective mainstream approximation of SVMs, which maximises the margin between the hyperplane (decision boundary) and individual data points in the feature space. It works well with high-dimensional datasets.\n",
    "- LinearSVC also has a `class_weight` parameter, and it was set to 'balanced'.\n",
    "- When testing the default version of LinearSVC, I received this error message: `ConvergenceWarning: Liblinear failed to converge, increase the number of iterations. Default max_iter is 1000.` I increased `max_iter` to 10,000.\n",
    "\n",
    "**Voting Ensemble (Weighted Probability Distribution Voting)**: this classifier combines the decisions of others, and I used the previous four classifiers given their high accuracies. I could not use soft voting (which predicts the class label based on the argmax of the sums of the predicted probabilities as opposed to predicted class labels) because LinearSVC does not produce predicted probabilities (it has no attribute `predict_proba`).\n",
    "\n",
    "*Note*: I tried two other ensemble classifiers, Random Forest and Gradient Boosting, which both represent more sophisticated versions of decision trees. The added sophistication also increases model complexity and required computational power, so neither was adopted in the end. Despite its added complexity, Random Forest is less accurate than the Decision Tree (when trained on 10,000 data points). It reaches a maximum accuracy at about 30 estimators:\n",
    "- 10 estimators: 0.9264685681977438\n",
    "- 20 estimators: 0.9307522122002345\n",
    "- 30 estimators: 0.9326311807290756\n",
    "- 40 estimators: 0.9319604758719968\n",
    "\n",
    "Scikit-learn's invaluable `Pipeline` was used to efficiently assemble classifiers. `DictVectorizer` was used to transform the feature dictionaries into vectors; the classifiers do not accept strings. \n",
    "\n",
    "Mean F1 scores after training on 50,000 (~1/4 of the data):\n",
    "- DT: 0.9572705006343316\n",
    "- NB: 0.92996301682618\n",
    "- LR: 0.9569817184051767\n",
    "- SVC: 0.961215343930895\n",
    "- VE: 0.9624448044664387\n",
    "\n",
    "Mean F1 scores after training on 90% of the training data (on a remote server with 128 GB RAM):\n",
    "- DT: 0.9660950649281544\n",
    "- NB: 0.9401225111526338\n",
    "- LR: 0.963039588811145\n",
    "- SVC: 0.9678034793207474"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_dt = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', DecisionTreeClassifier(random_state=0, criterion='entropy'))])\n",
    " \n",
    "clf_dt.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_dt = clf_dt.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_dt, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_nb = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', MultinomialNB(alpha=0.01))])\n",
    " \n",
    "clf_nb.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_nb = clf_nb.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_nb, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_lr = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', LogisticRegression(random_state=0, class_weight='balanced', solver='liblinear'))])\n",
    " \n",
    "clf_lr.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_lr = clf_lr.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_lr, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_svc = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                    ('classifier', LinearSVC(random_state=0, class_weight='balanced', max_iter=10000))])\n",
    " \n",
    "clf_svc.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_svc = clf_svc.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_svc, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_ve = VotingClassifier(estimators=[('dt', clf_dt), \n",
    "                                      ('nb', clf_nb), \n",
    "                                      ('lr', clf_lr), \n",
    "                                      ('svc', clf_svc)], voting='hard')\n",
    " \n",
    "clf_ve.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_ve = clf_ve.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_ve, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Compare Classifier Predictions and Accuracies After 10-fold Cross-validation\n",
    "\n",
    "The first ten predictions are identical for all of the models, and fully accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions = [predicted_dt, predicted_nb, predicted_lr, predicted_svc, predicted_ve]\n",
    "\n",
    "for prediction in model_predictions:\n",
    "    print(prediction[:10])\n",
    "\n",
    "print(y_test[:10].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [clf_dt, clf_nb, clf_lr, clf_svc, clf_ve]\n",
    "names = ['Decision Tree', 'Na√Øve Bayes', 'Linear SVC', 'Logistic Regression', 'Voting Ensemble']\n",
    "\n",
    "cv_df = pd.DataFrame(index=range(10 * len(models)))\n",
    "entries = []\n",
    "\n",
    "for model, name in zip(models, names):\n",
    "    model_name = name\n",
    "#    print(model_name)\n",
    "    accuracies = cross_val_score(model, X[:50000], y[:50000], cv=10, scoring='f1_weighted')\n",
    "    for fold_idx, accuracy in enumerate(accuracies):\n",
    "#        print(fold_idx, ':', accuracy)\n",
    "        entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n",
    "cv_df.to_csv('classifier_accuracies_chunking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,6)) \n",
    "f1_boxplot = sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n",
    "sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=8, jitter=True, edgecolor='gray', linewidth=2)\n",
    "plt.title('Boxplot of Chunking Classifier F1 Scores After 10-fold Cross-validation')\n",
    "plt.ylabel('Accuracy (F1 Score)')\n",
    "plt.xlabel('Model')\n",
    "plt.show()\n",
    "fig = f1_boxplot.get_figure()\n",
    "fig.savefig('Boxplot of Chunking Classifier F1 Scores.jpg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean F1 Scores\")\n",
    "print(cv_df.groupby('model_name').accuracy.mean())\n",
    "print(\"Standard Deviation\")\n",
    "print(cv_df.groupby('model_name').accuracy.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VI. Compute a Classification Report and Confusion Matrix for the Best Classifier\n",
    "\n",
    "The Voting Ensemble and Logistic Regression classifiers have the best performance after ten-fold cross-validation: a mean F1 score of 0.96 with a SD close to 0! The Voting Ensemble was chosen over the LR classifier because the boxplot reveals that the interquartile range of its accuracies is narrower, and because it incorporates the decisions and balances out the weaknesses of all four classifiers.\n",
    "\n",
    "A classification report showing average accuracies from all ten folds of the VE is computed. There are three reported averages: micro (averaging the total true positives, false negatives, and false positives), macro (averaging the unweighted mean per label), and weighted (averaging the support-weighted mean per label). Macro average can be disregarded, as there is extreme class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = []\n",
    "pred = []\n",
    "\n",
    "def cv_avg_classification_report(y_test, predicted_ve):\n",
    "    orig.extend(y_test)\n",
    "    pred.extend(predicted_ve)\n",
    "    return metrics.f1_score(y_test, predicted_ve, average='weighted')\n",
    "\n",
    "cross_val_score(clf_ve, X[:50000], y[:50000], cv=10, scoring=metrics.make_scorer(cv_avg_classification_report))\n",
    "print(metrics.classification_report(orig, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = metrics.confusion_matrix(y_test, predicted_ve)\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "conf_mat_heatmap = sns.heatmap(conf_mat, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix for Voting Ensemble Chunking Classifier')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()\n",
    "fig = conf_mat_heatmap.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VII. Train and Save the Best Classifier\n",
    "\n",
    "The Voting Ensemble classifier was trained on the full training dataset and saved as a pickle file. This was run on a remove server with 128 GB of RAM due to memory issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ve.fit(X, y)\n",
    "save_classifier = open('chunking_clf_trained.pickle', 'wb') \n",
    "pickle.dump(clf_ve, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_classifier = open('chunking_clf_trained.pickle', 'rb') \n",
    "chunking_clf_trained = pickle.load(saved_classifier)\n",
    "saved_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the classifier has been trained on all tweets: the F1 score is very close to 1, as it's been tested on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ve_trained = chunking_clf_trained.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_ve_trained, average='weighted'))\n",
    "print(metrics.classification_report(y_test, predicted_ve_trained))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VIII. Future Work/Model Refinement\n",
    "\n",
    "The accuracies produced by these classifiers are extremely high, but they were trained on ground truth POS tags, so will necessarily be an overinflation of classifier performance on unlabelled text. \n",
    "\n",
    "**Other Classification Methods**\n",
    "\n",
    "The classifiers tested here all have a fixed decision-making approach: the chunk labels that are made cannot be changed. An alternative approach suggested by the [NLTK book](https://www.nltk.org/book/ch06.html) is represented by **transformational joint classifiers**, which create initial label assignments that are iteratively refined if inconsistencies are found between related inputs. \n",
    "\n",
    "Another solution suggested by the book is assigning scores to all possible chunk sequences, and choosing the sequence with the highest overall score. This is represented by **Hidden Markov Models, Maximum Entropy Markov Models, and Linear-Chain Conditional Random Fields**. Rather than finding the single best tag for a given token, they generate a probability distribution over tags (i.e., they are probabilistic as opposed to deterministic). These probabilities are combined to calculate probability scores for tag sequences, and the tag sequence with the highest probability is chosen. To handle the enormous number of possible tag sequences, only the most recent tag is examined, and dynamic programming is used. For each consecutive token index i, a score is computed for each possible current and previous tag.\n",
    "\n",
    "And of course, there is the deep learning approach. **Long short-term memory networks (LSTMs)**, a type of deep bi-directional RNNs, are well-suited for chunking because they take sequences into account. [S√∏gaard and Goldberg (2016)](http://anthology.aclweb.org/P16-2038) have used this model for a multi-task learning architecture that combines POS tagging, syntactic chunking, and Combinatory Category Grammar (CCG) supertagging. As these tasks share a 'lot of substructure', it makes sense for the models to share parameters. POS supervision is best at the innermost rather than the outermost layer, because POS tagging represents a lower level task that feeds into higher level tasks. PyTorch is a great library for [LSTMs and sequence models](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html#lstm-s-in-pytorch).\n",
    "\n",
    "**Qualitative Analysis of Misclassifications / Feature Engineering**\n",
    "\n",
    "Misclassifications can be scrutinised to determine if there are any consistent patterns that the classifiers are overlooking. This information can, in turn, be used to refine feature engineering - arguably far more important than selecting sophisticated models.\n",
    "\n",
    "This relates to *focussing on different chunk types*, which have different characteristics. It is most important to examine the chunk types that had the lowest F1 scores (this is detailed in the classification report), as improving accuracies for these chunks has the largest impact on overall accuracy. Are there certain characteristics of these chunk types not captured by feature engineering? Neither of the two instances of I-PRT was classified correctly, so its F1 score was 0.00. The next lowest chunk F1 scores are for B-INTJ (0.29) and B-CONJP (0.40). The low accuracies make sense, as these were among the least frequent tags in the dataset. \n",
    "\n",
    "**Memory / Out of Core Learning **\n",
    "\n",
    "Fitting the classifiers requires a lot of memory. A more memory-effective solution would be to train by iterating over sections of the dataset at a time. Scikit-learn has incremental classifiers that learn from batches via the `partial_fit` method. It should be noted, however, that classifier accuracies improved only marginally (by about one percentage point) when trained on 90% of the training data vs. 25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
