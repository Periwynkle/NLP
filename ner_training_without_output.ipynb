{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "import string\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(font_scale=1.25)\n",
    "sns.set(style='white')\n",
    "sns.set(style='whitegrid', color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Import and Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data = 'Datasets/ner_dataset.txt'\n",
    "df = pd.read_csv(ner_data, sep=' ', header=None)\n",
    "df.columns = ['token', 'pos_tag', 'chunk_tag', 'ne_tag']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the rows with missing values: 6 are missing tokens, and 2,818 are missing named entity tags. These are presumably the empty lines after every sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_columns = df.columns[df.isnull().any()]\n",
    "print(df[null_columns].isnull().sum())\n",
    "\n",
    "df[df.isnull().any(axis=1)][null_columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True) # Reset the index so that it reflects the length of the dataframe and its columns. \n",
    "df.info() # Confirm that the missing values have been dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.pos_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,8))\n",
    "plt.title('POS Tag Frequency')\n",
    "sns.countplot(data=df, x='pos_tag');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.chunk_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "plt.title('Chunk Tag Frequency')\n",
    "sns.countplot(data=df, x='chunk_tag');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vast majority of tokens are not named entities: 210,679 out of 253,321 (83.17%). This is one baseline for classifier accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ne_tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.title('Named Entity Tag Frequency')\n",
    "sns.countplot(data=df, x='ne_tag');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the tags for feature engineering (what are the qualities/patterns that certain tags have?), generate examples of each.\n",
    "\n",
    "General named entity feature trends: capitalisation; POS tag is noun of some kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.ne_tag=='B-ORG'].sample(20) # First word in organisation name. Some tokens are abbreviations in all caps (e.g., 'EU').\n",
    "#df[df.ne_tag=='I-ORG'].sample(20) # Second word in organisation name.\n",
    "#df[df.ne_tag=='B-MISC'].sample(20) # Not sure how this differs from B-ORG. Many are adjectives.\n",
    "#df[df.ne_tag=='I-MISC'].sample(20) # Second part of B-MISC.\n",
    "df[df.ne_tag=='B-PER'].sample(20) # First names of people.\n",
    "#df[df.ne_tag=='I-PER'].sample(20) # Surnames of people. Mostly capitalised but not always (e.g., 'van').\n",
    "#df[df.ne_tag=='B-LOC'].sample(20) # Country and city names.\n",
    "#df[df.ne_tag=='I-LOC'].sample(20) # Second part of country and city names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Feature Engineering and Prepare Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(token, index, pos_tag, ne_tag):\n",
    "    first_letter = token[index][0]\n",
    "    features = {'token': token[index],\n",
    "                'pos': pos_tag[index],\n",
    "#                'chunk': chunk_tag[index],\n",
    "                'prev_token': '' if index == 0 else token[index-1],\n",
    "                'prev_pos': '' if index == 0 else pos_tag[index-1],\n",
    "                'prev_ne': '' if index == 0 else ne_tag[index-1],\n",
    "                'next_token': '' if index == len(df.token)-1 else token[index+1],\n",
    "                'next_pos': '' if index == len(df.token)-1 else pos_tag[index+1],\n",
    "                'prev_prev_token': '' if index == 0 or index == 1 else token[index-2],\n",
    "                'prev_prev_pos': '' if index == 0 or index == 1 else pos_tag[index-2],\n",
    "                'next_next_token': '' if index == len(df.token)-1 or index == len(df.token)-2 else token[index+2],\n",
    "                'next_next_pos': '' if index == len(df.token)-1 or index == len(df.token)-2 else pos_tag[index+2],\n",
    "                'is_capitalized': first_letter.upper() in string.ascii_uppercase and first_letter.upper() == first_letter,\n",
    "                'is_numeric': token[index].isdigit(),                \n",
    "                'is_all_caps': token[index].upper() == token[index],\n",
    "                'caps_inside': token[index][1:].lower() != token[index][1:]\n",
    "                }\n",
    "    return features\n",
    "    \n",
    "X = []\n",
    "\n",
    "for index in range(len(df.token)):\n",
    "    X.append(features(df.token, index, df.pos_tag, df.ne_tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['ne_tag']\n",
    "\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, df.index, test_size=0.1, random_state=0)\n",
    "\n",
    "print(\"Size of training set (POS tags):\", len(X_train)) \n",
    "print(\"Size of test set (POS tags):\", len(X_test)) \n",
    "print(\"Size of training set (chunk tags):\", len(y_train)) \n",
    "print(\"Size of test set (chunk tags):\", len(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Classifier Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token, pos: 0.8932555327159474\n",
    "# token, chunk: 0.8494862890783083 (took a LOT longer)\n",
    "# token, pos, chunk: 0.8957114704338746 (chunk doesn't help)\n",
    "# token, pos, previous token/pos: 0.8995017958096173\n",
    "# token, pos, previous token/pos, next token/pos: 0.9055109354730966\n",
    "# token, pos, previous token/pos, next token/pos, is_capitalised: 0.9115229634875965\n",
    "# token, pos, previous token/pos, previous previous token/pos, next token/pos, next next token/pos, is_capitalised: 0.9128023148431639\n",
    "# '', is_numeric: 0.9127205040729689\n",
    "# '', is_all_caps: 0.9132049059426229\n",
    "# '', caps_inside: 0.9142095785522023\n",
    "# ', prev_ne: 0.9387959473000405\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "clf_dt = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', DecisionTreeClassifier(random_state=0, criterion='entropy'))])\n",
    " \n",
    "clf_dt.fit(X_train[:10000], y_train[:10000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_dt = clf_dt.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_dt, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_nb = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', MultinomialNB(alpha=0.01))])\n",
    " \n",
    "clf_nb.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_nb = clf_nb.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_nb, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_lr = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                   ('classifier', LogisticRegression(random_state=0, class_weight='balanced', solver='liblinear'))])\n",
    " \n",
    "clf_lr.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_lr = clf_lr.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_lr, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "clf_svc = Pipeline([('vectorizer', DictVectorizer(sparse=False)),\n",
    "                    ('classifier', LinearSVC(random_state=0, class_weight='balanced', max_iter=10000))])\n",
    " \n",
    "clf_svc.fit(X_train[:50000], y_train[:50000])\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Total time:\", end_time-start_time)\n",
    "\n",
    "predicted_svc = clf_svc.predict(X_test)\n",
    "print(\"Mean F1 score (weighted):\", metrics.f1_score(y_test, predicted_svc, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, predicted_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_classifier = open('ne_clf_svc.pickle', 'wb') \n",
    "pickle.dump(clf_svc, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
